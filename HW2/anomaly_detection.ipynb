{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b64370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "from pytorch_msssim import ssim\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b475564",
   "metadata": {},
   "source": [
    "#### Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93491dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './mvtec_anomaly_detection/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a7886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Training Set: \\t\\t' + ', '.join(train_set))\n",
    "# print('Testing Folders: \\t' + ', '.join(test_folders))\n",
    "# print('Ground Truth Folders \\t' + ', '.join(ground_truth_folders))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6757540",
   "metadata": {},
   "source": [
    "# Pre-Processing & Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de3e118",
   "metadata": {},
   "source": [
    "#### Read image and convert it to gray image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d7dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width, img_channels = 256, 256, 1\n",
    "X, y_good, y_bad, ground_truth = [], [], [], []\n",
    "all_items = os.listdir(dataset_path)\n",
    "class_names = [item for item in all_items \n",
    "               if os.path.isdir(os.path.join(dataset_path, item))]\n",
    "\n",
    "for class_name in class_names:\n",
    "    train_path = dataset_path + class_name + '/train/good/'\n",
    "\n",
    "    test_path = dataset_path + class_name + '/test/'\n",
    "\n",
    "    ground_truth_path = dataset_path + class_name + '/ground_truth/'\n",
    "\n",
    "    train_set = os.listdir(train_path)\n",
    "\n",
    "    test_folders = os.listdir(test_path)\n",
    "\n",
    "    ground_truth_folders = os.listdir(ground_truth_path)\n",
    "    \n",
    "    for img_name in train_set:\n",
    "        img_path = train_path + img_name\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((img_height, img_width))\n",
    "        img_gray = img.convert('L')\n",
    "        img_gray = transform_pipeline(img_gray)\n",
    "        X.append(img_gray)\n",
    "\n",
    "    for test_folder in test_folders:\n",
    "        test_folder_path = test_path + test_folder\n",
    "\n",
    "        for img_name in sorted(os.listdir(test_folder_path)):\n",
    "            img_path = test_folder_path + '/' + img_name\n",
    "            img = Image.open(img_path)\n",
    "            img = img.resize((img_height, img_width))\n",
    "            img_gray = img.convert('L')\n",
    "            img_gray = transform_pipeline(img_gray)\n",
    "\n",
    "            if test_folder == 'good':\n",
    "                y_good.append(img_gray)\n",
    "            else:\n",
    "                y_bad.append(img_gray)\n",
    "            \n",
    "    for ground_truth_folder in ground_truth_folders:\n",
    "        ground_truth_folder_path = ground_truth_path + ground_truth_folder\n",
    "\n",
    "        for img_name in sorted(os.listdir(ground_truth_folder_path)):\n",
    "            img_path = ground_truth_folder_path + '/' + img_name\n",
    "            img = Image.open(img_path)\n",
    "            img = img.resize((img_height, img_width))\n",
    "            img_gray = img.convert('L')\n",
    "            img_gray = transform_pipeline(img_gray)\n",
    "            ground_truth.append(img_gray)\n",
    "\n",
    "print(\"Training: \\t\\t\", np.shape(X))\n",
    "print(\"Good Testing: \\t\\t\", np.shape(y_good))\n",
    "print(\"Bad Testing: \\t\\t\", np.shape(y_bad))\n",
    "print(\"Ground Truth: \\t\\t\", np.shape(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1438276",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f62484",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.stack(X)\n",
    "y_good_tensor = torch.stack(y_good)\n",
    "y_bad_tensor = torch.stack(y_bad)\n",
    "\n",
    "n_features = img_height * img_width\n",
    "X_train_features = X_tensor.reshape(-1, n_features)\n",
    "y_good_features = y_good_tensor.reshape(-1, n_features)\n",
    "y_bad_features = y_bad_tensor.reshape(-1, n_features)\n",
    "\n",
    "X_train_features = X_train_features.detach().cpu().numpy()\n",
    "y_good_features = y_good_features.detach().cpu().numpy()\n",
    "y_bad_features = y_bad_features.detach().cpu().numpy()\n",
    "\n",
    "print(\"Training feature shape: \\t\", X_train_features.shape)\n",
    "print(\"Good test feature shape: \\t\", y_good_features.shape)\n",
    "print(\"Bad test feature shape: \\t\", y_bad_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training One-Class SVM...\")\n",
    "svm = OneClassSVM(kernel='rbf', nu=0.1, gamma='auto')\n",
    "\n",
    "svm.fit(X_train_features)\n",
    "print(\"SVM training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7721fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_features = np.concatenate((y_good_features, y_bad_features), axis=0)\n",
    "\n",
    "labels_good = np.zeros(y_good_features.shape[0])\n",
    "labels_bad = np.ones(y_bad_features.shape[0])\n",
    "y_true_labels = np.concatenate((labels_good, labels_bad), axis=0)\n",
    "\n",
    "print(\"Total test features shape: \\t\", X_test_features.shape)\n",
    "print(\"Total test labels shape: \\t\", y_true_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652401c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = svm.predict(X_test_features)\n",
    "\n",
    "y_pred_labels = [0 if p == 1 else 1 for p in svm_predictions]\n",
    "\n",
    "accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
    "auc = roc_auc_score(y_true_labels, y_pred_labels)\n",
    "\n",
    "print(\"--- Evaluation Results ---\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"AUC Score: {auc:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2904df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('submit_SVM.csv', mode='w', newline='') as submit_file:\n",
    "    csv_writer = csv.writer(submit_file)\n",
    "    header = ['id', 'prediction']\n",
    "    print(header)\n",
    "    csv_writer.writerow(header)\n",
    "    for i in range(svm_predictions.shape[0]):\n",
    "        row = [str(i), 0 if svm_predictions[i] == -1 else 1]\n",
    "        csv_writer.writerow(row)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93788521",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf26cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "print(\"Fitting KNN model...\")\n",
    "knn = NearestNeighbors(n_neighbors=k, n_jobs=-1)\n",
    "knn.fit(X_train_features)\n",
    "print(X_train_features)\n",
    "print(\"Model fitting complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83207b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = knn.kneighbors(X_test_features)\n",
    "\n",
    "anomaly_scores = np.mean(distances, axis=1)\n",
    "\n",
    "print(f\"Calculated {len(anomaly_scores)} anomaly scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e265032",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distances, _ = knn.kneighbors(X_train_features)\n",
    "train_anomaly_scores = np.mean(train_distances, axis=1)\n",
    "\n",
    "threshold = np.percentile(train_anomaly_scores, 95)\n",
    "print(f\"Anomaly Threshold set to: {threshold:.4f}\")\n",
    "\n",
    "y_pred_labels = (anomaly_scores > threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
    "auc = roc_auc_score(y_true_labels, anomaly_scores)\n",
    "print(\"\\n--- Evaluation result ---\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"AUC Score: {auc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0002f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submit_KNN.csv', mode='w', newline='') as submit_file:\n",
    "    csv_writer = csv.writer(submit_file)\n",
    "    header = ['id', 'prediction']\n",
    "    print(header)\n",
    "    csv_writer.writerow(header)\n",
    "    for i in range(svm_predictions.shape[0]):\n",
    "        row = [str(i), y_pred_labels[i]]\n",
    "        csv_writer.writerow(row)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b8742",
   "metadata": {},
   "source": [
    "# AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b6a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), # [N, 1, 256, 256] -> [N, 16, 128, 128]\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # [N, 16, 128, 128] -> [N, 32, 64, 64]\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # [N, 32, 64, 64] -> [N, 64, 32, 32]\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), # [N, 64, 32, 32] -> [N, 128, 16, 16]\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1), # [N, 128, 16, 16] -> [N, 256, 8, 8]\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1), # [N, 256, 8, 8] -> [N, 128, 16, 16]\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1), # [N, 128, 16, 16] -> [N, 64, 32, 32]\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1), # [N, 64, 32, 32] -> [N, 32, 64, 64]\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1), # [N, 32, 64, 64] -> [N, 16, 128,128]\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1), #[N, 16, 128, 128] -> #[N, 1, 256, 256]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "train_dataset = TensorDataset(X_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = AutoEncoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"--- Starting Training ---\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        images = data[0].to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * images.size(0)\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.6f}\")\n",
    "\n",
    "print(\"--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e29d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "y_good_tensor = y_good_tensor.to(device)\n",
    "y_bad_tensor = y_bad_tensor.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_good = model(y_good_tensor)\n",
    "    reconstructed_bad = model(y_bad_tensor)\n",
    "\n",
    "    test_loss = criterion(reconstructed_good, y_good_tensor)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='none')\n",
    "\n",
    "error_good = loss_fn(reconstructed_good, y_good_tensor)\n",
    "scores_good = torch.mean(error_good, dim=(1, 2, 3))\n",
    "\n",
    "error_bad = loss_fn(reconstructed_bad, y_bad_tensor)\n",
    "scores_bad = torch.mean(error_bad, dim=(1, 2, 3))\n",
    "\n",
    "anomaly_scores_tensor = torch.cat([scores_good, scores_bad])\n",
    "anomaly_scores_numpy = anomaly_scores_tensor.cpu().numpy()\n",
    "\n",
    "auc = roc_auc_score(y_true_labels, anomaly_scores_numpy)\n",
    "\n",
    "print(\"--- AutoEncoder Evaluation ---\")\n",
    "print(f\"AUC Score: {auc:.4f}\")\n",
    "print(f\"Testing Loss: {test_loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e618f",
   "metadata": {},
   "source": [
    "# AE + KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f87ebd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70024630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walking through ./Dataset/train/ to find 'good' training images...\n",
      "Found 3629 normal training images.\n",
      "Walking through ./Dataset/test/ to find 'good' training images...\n",
      "Found 1725 normal training images.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define your transformations ---\n",
    "# We resize, convert to grayscale, and turn into a tensor\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.Grayscale(num_output_channels=1),\n",
    "    \n",
    "    # --- Data Augmentations Added ---\n",
    "    # 50% chance of flipping the image horizontally\n",
    "    T.RandomHorizontalFlip(p=0.5), \n",
    "    \n",
    "    # 50% chance of flipping the image vertically\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    \n",
    "    # Rotate the image by a random amount (e.g., up to 20 degrees)\n",
    "    T.RandomRotation(20),\n",
    "    \n",
    "    # Slightly change brightness and contrast\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    # ----------------------------------\n",
    "    \n",
    "    T.ToTensor(),\n",
    "    # It's also a good practice to normalize your data\n",
    "    # T.Normalize(mean=[0.5], std=[0.5]) \n",
    "])\n",
    "\n",
    "# --- IMPORTANT ---\n",
    "# Your test_transform should NOT have augmentations\n",
    "test_transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.Grayscale(num_output_channels=1),\n",
    "    T.ToTensor(),\n",
    "    # T.Normalize(mean=[0.5], std=[0.5]) # Add if you normalize in training\n",
    "])\n",
    "\n",
    "# --- 2. Create your custom Dataset class ---\n",
    "# (This class is good, no changes needed from last time)\n",
    "class MVTecDataset(Dataset):\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # For this dataset, we only need the image for training\n",
    "        return (image,) # Return as a tuple\n",
    "\n",
    "# --- 3. Load your REAL training file paths ---\n",
    "train_dir = './Dataset/train/'\n",
    "train_files = []\n",
    "image_extensions = ('.png', '.jpg', '.jpeg', '.tif', '.bmp')\n",
    "\n",
    "print(f\"Walking through {train_dir} to find 'good' training images...\")\n",
    "\n",
    "# os.walk will go through all subfolders (bottle, carpet, etc.)\n",
    "for root, dirs, files in os.walk(train_dir):\n",
    "    # The 'good' folder contains the normal training images\n",
    "    for file in files:\n",
    "        if file.lower().endswith(image_extensions):\n",
    "            train_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(train_files)} normal training images.\")\n",
    "\n",
    "# --- 4. Create your REAL Dataset and DataLoader ---\n",
    "train_dataset = MVTecDataset(train_files, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "test_dir = './Dataset/test/'\n",
    "test_files = []\n",
    "image_extensions = ('.png', '.jpg', '.jpeg', '.tif', '.bmp')\n",
    "\n",
    "print(f\"Walking through {test_dir} to find 'good' training images...\")\n",
    "\n",
    "# os.walk will go through all subfolders (bottle, carpet, etc.)\n",
    "for root, dirs, files in os.walk(test_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(image_extensions):\n",
    "            test_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(test_files)} normal training images.\")\n",
    "# (Assuming 'test_files' is your list of test image paths)\n",
    "test_dataset = MVTecDataset(test_files, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9116b18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "--- Starting AE Training ---\n",
      "Epoch [1/50], Loss: 0.038701\n",
      "Epoch [2/50], Loss: 0.015337\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     74\u001b[39m     epoch_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/master/1st_year/Data-Mining/HW2/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/master/1st_year/Data-Mining/HW2/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/master/1st_year/Data-Mining/HW2/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mMVTecDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     47\u001b[39m image = Image.open(img_path)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# For this dataset, we only need the image for training\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (image,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/master/1st_year/Data-Mining/HW2/.venv/lib/python3.13/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/master/1st_year/Data-Mining/HW2/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/master/1st_year/Data-Mining/HW2/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/master/1st_year/Data-Mining/HW2/.venv/lib/python3.13/site-packages/torchvision/transforms/transforms.py:354\u001b[39m, in \u001b[36mResize.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m    347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m \u001b[33;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/master/1st_year/Data-Mining/HW2/.venv/lib/python3.13/site-packages/torchvision/transforms/functional.py:477\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation, max_size, antialias)\u001b[39m\n\u001b[32m    475\u001b[39m         warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    476\u001b[39m     pil_interpolation = pil_modes_mapping[interpolation]\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F_t.resize(img, size=output_size, interpolation=interpolation.value, antialias=antialias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/master/1st_year/Data-Mining/HW2/.venv/lib/python3.13/site-packages/torchvision/transforms/_functional_pil.py:253\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) == \u001b[32m2\u001b[39m):\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/master/1st_year/Data-Mining/HW2/.venv/lib/python3.13/site-packages/PIL/Image.py:2301\u001b[39m, in \u001b[36mImage.resize\u001b[39m\u001b[34m(self, size, resample, box, reducing_gap)\u001b[39m\n\u001b[32m   2298\u001b[39m     im = im.resize(size, resample, box)\n\u001b[32m   2299\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m im.convert(\u001b[38;5;28mself\u001b[39m.mode)\n\u001b[32m-> \u001b[39m\u001b[32m2301\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reducing_gap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resample != Resampling.NEAREST:\n\u001b[32m   2304\u001b[39m     factor_x = \u001b[38;5;28mint\u001b[39m((box[\u001b[32m2\u001b[39m] - box[\u001b[32m0\u001b[39m]) / size[\u001b[32m0\u001b[39m] / reducing_gap) \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/master/1st_year/Data-Mining/HW2/.venv/lib/python3.13/site-packages/PIL/ImageFile.py:390\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    387\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    389\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- 0. Model & Helper Function Definitions ---\n",
    "\n",
    "# This AE is for 1-channel (grayscale) 256x256 images\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, latent_dims=128):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        # --- Encoder ---\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # -> 32x128x128\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), # -> 64x64x64\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), # -> 128x32x32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), # -> 256x16x16\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 16 * 16, latent_dims) # -> 128\n",
    "        )\n",
    "        \n",
    "        # --- Decoder ---\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dims, 256 * 16 * 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (256, 16, 16)),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # -> 128x32x32\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # -> 64x64x64\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1), # -> 32x128x128\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1), # -> 1x256x256\n",
    "            nn.Sigmoid() # Output images between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed\n",
    "\n",
    "def get_features(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Helper function to extract encoder features for an entire dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images = data[0].to(device)\n",
    "            features = model.encoder(images)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(all_features, axis=0)\n",
    "\n",
    "# --- 1. Setup & Train the Autoencoder ---\n",
    "\n",
    "# Hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 50 # Increase this for real data\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "latent_dims = 128\n",
    "\n",
    "model = AutoEncoder(latent_dims=latent_dims).to(device)\n",
    "criterion = nn.MSELoss() # Simple MSE is fine for training the feature extractor\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"--- Starting AE Training ---\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        images = data[0].to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * images.size(0)\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataset)\n",
    "    # if (epoch + 1) % 10 == 0:\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.6f}\")\n",
    "\n",
    "print(\"--- AE Training Complete ---\")\n",
    "\n",
    "\n",
    "# --- 2. k-NN Setup (Build the \"Map of Normal\") ---\n",
    "\n",
    "print(\"\\n--- Building k-NN Feature Map ---\")\n",
    "# Create a dataloader for the *full* training set (no shuffling)\n",
    "train_loader_full = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 1. Get all features from the \"normal\" training data\n",
    "X_train_features = get_features(model, train_loader_full, device)\n",
    "print(f\"Created training feature map with shape: {X_train_features.shape}\")\n",
    "\n",
    "# 2. Fit the k-NN model on these \"normal\" features\n",
    "knn = NearestNeighbors(n_neighbors=5, n_jobs=-1)\n",
    "knn.fit(X_train_features)\n",
    "print(\"k-NN model fitted on training features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757515ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Evaluating Test Set ---\")\n",
    "# 1. Get features for all test images\n",
    "test_features = get_features(model, test_loader, device)\n",
    "print(f\"Created test feature map with shape: {test_features.shape}\")\n",
    "\n",
    "# 2. Get distances to nearest neighbors in the \"normal\" map\n",
    "distances, _ = knn.kneighbors(test_features)\n",
    "\n",
    "# 3. Anomaly score = mean distance to neighbors\n",
    "anomaly_scores = np.mean(distances, axis=1)\n",
    "print(f\"Calculated {len(anomaly_scores)} anomaly scores.\")\n",
    "\n",
    "# --- 4. Set Threshold & Get Results ---\n",
    "\n",
    "# 1. Get scores for the *training* data to find a \"normal\" threshold\n",
    "train_distances, _ = knn.kneighbors(X_train_features)\n",
    "train_scores = np.mean(train_distances, axis=1)\n",
    "\n",
    "# 2. Set the threshold at the 95th percentile of \"normal\" scores\n",
    "threshold = np.percentile(train_scores, 95)\n",
    "print(f\"Anomaly threshold set to: {threshold:.6f}\")\n",
    "\n",
    "# 3. Make predictions\n",
    "predictions = (anomaly_scores > threshold).astype(int)\n",
    "\n",
    "# --- 5. Print Results ---\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Predictions (first 20): {predictions[:20]}\")\n",
    "print(f\"Total anomalies predicted: {np.sum(predictions)} / {len(predictions)}\")\n",
    "\n",
    "# If you have labels, you can calculate the AUC\n",
    "# We use the raw scores, not the 0/1 predictions, for AUC\n",
    "\n",
    "# Visualize a test image and its reconstruction\n",
    "print(\"Visualizing a test image...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # --- THIS IS THE FIX ---\n",
    "    # Get the 150th image tensor directly from your dataset\n",
    "    # test_dataset[150] returns a tuple (image_tensor,)\n",
    "    # so we take the first element [0]\n",
    "    img_tensor = test_dataset[150][0].unsqueeze(0).to(device)\n",
    "    # -----------------------\n",
    "\n",
    "    recon = model(img_tensor)\n",
    "\n",
    "    img = img_tensor.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
    "    rec_img = recon.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax1.imshow(img, cmap='gray')\n",
    "    ax1.set_title(\"Original Image (Index 150)\")\n",
    "    ax1.axis('off')\n",
    "\n",
    "    ax2.imshow(rec_img, cmap='gray')\n",
    "    ax2.set_title(\"Reconstructed\")\n",
    "    ax2.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b4d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submit_AutoEncoder.csv', mode='w', newline='') as submit_file:\n",
    "    csv_writer = csv.writer(submit_file)\n",
    "    header = ['id', 'prediction']\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    # This loop is now correct\n",
    "    for i in range(len(predictions)):\n",
    "        row = [str(i), predictions[i]]\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "print(\"--- Submission file 'submit_AutoEncoder.csv' created. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
